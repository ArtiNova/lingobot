{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKa3S1pwd30j"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/wrong_data.csv')"
      ],
      "metadata": {
        "id": "zRU03FEMeDot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.iloc[10, 2]"
      ],
      "metadata": {
        "id": "qMPQyU6_PZ5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "g0ZVQ75JrkEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install happytransformer"
      ],
      "metadata": {
        "id": "_DBlVBWSeLri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "VObXwhsTZ0fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_data = []"
      ],
      "metadata": {
        "id": "r7JHdvxbqhrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, j in list(zip(list(dataset['wrong']), list(dataset['correct']))):\n",
        "  json_data.append({'input_text' : i, 'output_text' : j})"
      ],
      "metadata": {
        "id": "QmAbD_NNqbj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json.dump(json_data, open('train.json', 'w'))"
      ],
      "metadata": {
        "id": "GtZLCgkgq3MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets"
      ],
      "metadata": {
        "id": "XZzV9DWyr_d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "3evZwQ9ks9pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = json.load(open('/content/drive/MyDrive/train.json'))[:20000]"
      ],
      "metadata": {
        "id": "UPx5qBChjsfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "40BQBiTKlkz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_l = json.load(open('/content/drive/MyDrive/train_new.json'))"
      ],
      "metadata": {
        "id": "S5wj3XmtkI4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "ygS7eqx3kNEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_extend = random.sample(data_l, 100000)"
      ],
      "metadata": {
        "id": "_WPDaERLlTVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "xiJvFDeSuydt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "import datasets\n",
        "import json\n",
        "\n",
        "# Custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_input_length, max_output_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_length = max_input_length\n",
        "        self.max_output_length = max_output_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data[index]\n",
        "        input_text = item['input_text']\n",
        "        output_text = item['output_text']\n",
        "\n",
        "        # Tokenize and truncate input and output\n",
        "        input_encoding = self.tokenizer(\n",
        "            input_text,\n",
        "            max_length=self.max_input_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "\n",
        "        output_encoding = self.tokenizer(\n",
        "            output_text,\n",
        "            max_length=self.max_output_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_encoding.input_ids.flatten(),\n",
        "            \"attention_mask\": input_encoding.attention_mask.flatten(),\n",
        "            \"labels\": output_encoding.input_ids.flatten(),\n",
        "        }\n",
        "\n",
        "# Load your data\n",
        "data_l.extend(json.load(open('/content/drive/MyDrive/new_data.json')))\n",
        "\n",
        "\n",
        "\n",
        "# Optional: Evaluate the model if you have an evaluation dataset\n",
        "# evaluation_dataset = ...\n",
        "# trainer.evaluate(eval_dataset=evaluation_dataset)\n"
      ],
      "metadata": {
        "id": "ORAXDL5Cuqjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = CustomDataset(random.sample(data_l, 100000), tokenizer, max_input_length, max_output_length)"
      ],
      "metadata": {
        "id": "IaRDmrwYmbg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = CustomDataset(data, tokenizer, max_input_length, max_output_length)"
      ],
      "metadata": {
        "id": "n0qtJEsPm0pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-base\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/mt5-base\")"
      ],
      "metadata": {
        "id": "fUXwSBzRw8RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 128  # You can adjust this based on your dataset\n",
        "max_output_length = 128  # You can adjust this based on your dataset\n",
        "#custom_dataset = CustomDataset(data[:400], tokenizer, max_input_length, max_output_length)"
      ],
      "metadata": {
        "id": "8kKhtXcvxwKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "train_data = random.sample(data, 7512)\n",
        "test_data = list(filter(lambda x : x not in train_data, data))"
      ],
      "metadata": {
        "id": "7vYwVm6nn_i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/wiki_extracts.csv')"
      ],
      "metadata": {
        "id": "pcBeuNu-0ZET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data = []\n",
        "for i, j in list(zip(list(df['input']), list(df['target']))):\n",
        "  eval_data.append({'input_text' : i, 'output_text' : j})"
      ],
      "metadata": {
        "id": "cw8wFOdE0wQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "train_data = random.sample(eval_data, 12000)\n",
        "test_data = list(filter(lambda x : x not in train_data, eval_data))"
      ],
      "metadata": {
        "id": "2oj8HU7FGc0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = CustomDataset(train_data, tokenizer, max_input_length, max_output_length)\n",
        "test_data = CustomDataset(test_data, tokenizer, max_input_length, max_output_length)"
      ],
      "metadata": {
        "id": "wBqbyurzoqUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "id": "e6v2t3x8ssf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('train_data.pkl', 'rb') as f:\n",
        "  train_data = pickle.load(f)\n",
        "with open('test_data.pkl', 'rb') as f:\n",
        "  test_data = pickle.load(f)"
      ],
      "metadata": {
        "id": "V0yvf3Q9VkUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"train_data.pkl\" , \"wb\") as f:\n",
        "  pickle.dump(train_data, f)\n",
        "with open(\"test_data.pkl\" , \"wb\") as f:\n",
        "  pickle.dump(test_data, f)"
      ],
      "metadata": {
        "id": "S80YOnmhU1Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = CustomDataset(eval_data, tokenizer, max_input_length, max_output_length)"
      ],
      "metadata": {
        "id": "if-oEBgd0RW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        ")\n",
        "\n",
        "# Initialize Trainer and fine-tune the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "hFvpyB81x0QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r results_v3.zip ./results/model ./results/tokenizer"
      ],
      "metadata": {
        "id": "we2ErtfjP92u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./results/model\")\n",
        "\n",
        "# You can also save the tokenizer\n",
        "tokenizer.save_pretrained(\"./results/tokenizer\")"
      ],
      "metadata": {
        "id": "64k-ra03x6na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r results.zip"
      ],
      "metadata": {
        "id": "zfLPgC7kQwuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp results_v5.zip /content/drive/MyDrive/results_v5.zip"
      ],
      "metadata": {
        "id": "_Z_H_2tU6ept"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree(\"./results/model\")\n",
        "shutil.rmtree(\"./results/tokenizer\")"
      ],
      "metadata": {
        "id": "mXup5_ezYKGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "9VnaGXXsP1k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"./results_v5/results/model/\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./results_v5/results/tokenizer/\")"
      ],
      "metadata": {
        "id": "Nx8y3ZzxPtE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "qluXhBE6dZxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ch = random.choice(data)\n",
        "inputs = tokenizer(\"\", return_tensors=\"pt\")\n",
        "max_length = 128  # Adjust as needed\n",
        "generated = model.generate(inputs[\"input_ids\"], max_length=max_length, num_return_sequences=1)\n",
        "corrected_text = tokenizer.decode(generated[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "CNbIaSK6Q9Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Prediction\\t\" + corrected_text)\n",
        "print(\"Expected\\t\" + ch['output_text'])\n",
        "print(\"Wrong text\\t\" + ch['input_text'])\n",
        "print(\"Corrected ? : \" + corrected_text == ch['output_text'])"
      ],
      "metadata": {
        "id": "Oyqf08voRLEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp /content/drive/MyDrive/results_v5.zip ."
      ],
      "metadata": {
        "id": "kaRES-FEFL_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"results_v5.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"./results_v5\")"
      ],
      "metadata": {
        "id": "dDA1dSVYFUfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CK5vsi9qR9nd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}